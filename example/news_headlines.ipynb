{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import multiprocessing\n",
    "from gensim.models import word2vec\n",
    "import pandas as pd\n",
    "from textDataset import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs: 7\n"
     ]
    }
   ],
   "source": [
    "cpu_count = 2*multiprocessing.cpu_count()-1\n",
    "print('Number of CPUs: {}'.format(cpu_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path2data = '../data/news_headlines/'\n",
    "\n",
    "text = {\n",
    "        'train': TextDataset(path2data, extension='.csv', sep=',', is_train = True),\n",
    "}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col = 'headline_text'\n",
    "text['train'].process_data(col = col, remove_stopw = True, remove_tags=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "      <th>subject</th>\n",
       "      <th>headline_text_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030303</td>\n",
       "      <td>unhooked brakes to blame for taiwan train disa...</td>\n",
       "      <td>news_headlines</td>\n",
       "      <td>{blame, train, taiwan, disaster, brakes, unhoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030918</td>\n",
       "      <td>oldest prisoner in tas released citing health</td>\n",
       "      <td>news_headlines</td>\n",
       "      <td>{prisoner, released, tas, oldest, citing, health}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030913</td>\n",
       "      <td>nine reportedly dead in portuguese plane crash</td>\n",
       "      <td>news_headlines</td>\n",
       "      <td>{crash, portuguese, reportedly, dead, nine, pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20031031</td>\n",
       "      <td>nurses welcome medicare rebate plan</td>\n",
       "      <td>news_headlines</td>\n",
       "      <td>{plan, welcome, medicare, nurses, rebate}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030930</td>\n",
       "      <td>un cuts its iraq staff</td>\n",
       "      <td>news_headlines</td>\n",
       "      <td>{iraq, staff, cuts, un}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text  \\\n",
       "0      20030303  unhooked brakes to blame for taiwan train disa...   \n",
       "1      20030918      oldest prisoner in tas released citing health   \n",
       "2      20030913     nine reportedly dead in portuguese plane crash   \n",
       "3      20031031                nurses welcome medicare rebate plan   \n",
       "4      20030930                             un cuts its iraq staff   \n",
       "\n",
       "          subject                                 headline_text_data  \n",
       "0  news_headlines  {blame, train, taiwan, disaster, brakes, unhoo...  \n",
       "1  news_headlines  {prisoner, released, tas, oldest, citing, health}  \n",
       "2  news_headlines  {crash, portuguese, reportedly, dead, nine, pl...  \n",
       "3  news_headlines          {plan, welcome, medicare, nurses, rebate}  \n",
       "4  news_headlines                            {iraq, staff, cuts, un}  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text['train'].data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = { 'train': text['train'].data[col + '_data'].values}\n",
    "X_train = sentences['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000001/1000001 [00:00<00:00, 2504641.78it/s]\n",
      "100%|██████████| 1000001/1000001 [00:00<00:00, 2483610.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 50   # Minimum word count                        \n",
    "num_workers = cpu_count  # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "\n",
    "\n",
    "W2Vmodel = word2vec.Word2Vec(workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "\n",
    "W2Vmodel.build_vocab([x for x in tqdm(X_train)])\n",
    "W2Vmodel.train([x for x in tqdm(X_train)], \\\n",
    "            total_examples=W2Vmodel.corpus_count, epochs=W2Vmodel.epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print ('Building tf-idf matrix ...')\n",
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform([x for x in X_train])\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "print ('vocab size : {}'.format(len(tfidf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildWordVector(model, tfidf, tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: \n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_train = np.concatenate([buildWordVector(W2Vmodel, tfidf, z, num_features) for z in map(lambda x: x, X_train)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Shape of train features:: {}'.format(f_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def elbow_rule(f_train, max_nb_cluster = 10, distortions_method='euclidean', plot=True):\n",
    "\n",
    "    # k means determine k\n",
    "    distortions = []\n",
    "\n",
    "    for k in range(1,max_nb_cluster):\n",
    "        print('Training K-means models for {} cluster/s...'.format(k))\n",
    "        kmeanModel = KMeans(n_clusters=k).fit(f_train)\n",
    "        kmeanModel.fit(f_train)\n",
    "        if distortions_method == 'euclidean':\n",
    "            distortions.append(sum(np.min(cdist(f_train, kmeanModel.cluster_centers_, 'euclidean'), \\\n",
    "                                          axis=1)) / f_train.shape[0])\n",
    "        #elif other distortion evaluation\n",
    "        \n",
    "    if plot:\n",
    "        # Plot the elbow\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.plot(range(1,max_nb_cluster), distortions, 'bx-')\n",
    "        plt.xlabel('k')\n",
    "        plt.ylabel('Distortion')\n",
    "        plt.title('Elbow Method')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow_rule(f_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
