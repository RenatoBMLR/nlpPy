{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing librares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from tqdm import tqdm\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from textDataset import *\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs: 7\n"
     ]
    }
   ],
   "source": [
    "cpu_count = 2*multiprocessing.cpu_count()-1\n",
    "print('Number of CPUs: {}'.format(cpu_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path2data = '../../'\n",
    "path2data = '../data/news_headlines/'\n",
    "\n",
    "text = {\n",
    "        'train': TextDataset(path2data, extension='.csv', sep=',', is_train = True),\n",
    "}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'headline_text'\n",
    "text['train'].process_data(col = col, remove_stopw = True, remove_tags=False, lemmalize = True, stem = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "      <th>subject</th>\n",
       "      <th>headline_text_data</th>\n",
       "      <th>nb_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030303</td>\n",
       "      <td>unhooked brakes to blame for taiwan train disa...</td>\n",
       "      <td>news_headlines</td>\n",
       "      <td>[unhook, brake, blame, taiwan, train, disast]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030918</td>\n",
       "      <td>oldest prisoner in tas released citing health</td>\n",
       "      <td>news_headlines</td>\n",
       "      <td>[oldest, prison, ta, releas, cite, health]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030913</td>\n",
       "      <td>nine reportedly dead in portuguese plane crash</td>\n",
       "      <td>news_headlines</td>\n",
       "      <td>[nine, reportedli, dead, portugues, plane, crash]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20031031</td>\n",
       "      <td>nurses welcome medicare rebate plan</td>\n",
       "      <td>news_headlines</td>\n",
       "      <td>[nurs, welcom, medicar, rebat, plan]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030930</td>\n",
       "      <td>un cuts its iraq staff</td>\n",
       "      <td>news_headlines</td>\n",
       "      <td>[un, cut, iraq, staff]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text  \\\n",
       "0      20030303  unhooked brakes to blame for taiwan train disa...   \n",
       "1      20030918      oldest prisoner in tas released citing health   \n",
       "2      20030913     nine reportedly dead in portuguese plane crash   \n",
       "3      20031031                nurses welcome medicare rebate plan   \n",
       "4      20030930                             un cuts its iraq staff   \n",
       "\n",
       "          subject                                 headline_text_data  nb_words  \n",
       "0  news_headlines      [unhook, brake, blame, taiwan, train, disast]         6  \n",
       "1  news_headlines         [oldest, prison, ta, releas, cite, health]         6  \n",
       "2  news_headlines  [nine, reportedli, dead, portugues, plane, crash]         6  \n",
       "3  news_headlines               [nurs, welcom, medicar, rebat, plan]         5  \n",
       "4  news_headlines                             [un, cut, iraq, staff]         4  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text['train'].data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset lenght: 1000001\n",
      "Dataset lenght: 643715\n"
     ]
    }
   ],
   "source": [
    "print('Dataset lenght: {}'.format(len(text['train'].data)))\n",
    "text['train'].data = text['train'].data.drop_duplicates(subset='headline_text', keep=\"last\")\n",
    "print('Dataset lenght: {}'.format(len(text['train'].data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "text['train'].data.publish_date = pd.to_datetime(text['train'].data.publish_date.astype(str),format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "text['train'].data.publish_date.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "text['train'].data.publish_date.dt.year.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options a given year (i.e: 2012,2013) or -1 for all 15 years\n",
    "year = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if year == -1:\n",
    "    sentences = { 'train': text['train'].data[col + '_data'].values}\n",
    "    X_train = sentences['train']\n",
    "else:\n",
    "    sentences = { 'train': text['train'].data[text['train'].data.publish_date.dt.year == year][col + '_data'].values}\n",
    "    X_train = sentences['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_extractor = 'word2Vec'\n",
    "#features_extractor = 'bow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Features extractor: {}'.format(features_extractor))\n",
    "\n",
    "if features_extractor == 'bow':\n",
    "    \n",
    "    print('Counting ocorrences of words...')\n",
    "\n",
    "    max_gram = 2\n",
    "    \n",
    "    print('ngram-range: {}'.format(str((1,max_gram))))\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                                 max_features = 300, ngram_range=(1,max_gram)) \n",
    "    f_train = vectorizer.fit_transform([\" \".join(x) for x in tqdm(X_train)])\n",
    "\n",
    "    #downscale weights for words using tf–idf: “Term Frequency times Inverse Document Frequency”.\n",
    "    print('Words downscaling using TF-IDF...')\n",
    "    \n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    f_train = tfidf_transformer.fit_transform(f_train)\n",
    "\n",
    "else:\n",
    "    \n",
    "    # Set values for various parameters\n",
    "    num_features = 300    # Word vector dimensionality                      \n",
    "    min_word_count = 50   # Minimum word count                        \n",
    "    num_workers = cpu_count  # Number of threads to run in parallel\n",
    "    context = 10          # Context window size                                                                                    \n",
    "    downsampling = 1e-3   # Downsample setting for frequent words (default value)\n",
    "\n",
    "\n",
    "    print('Creating Word2Vec Model...')\n",
    "    W2Vmodel = word2vec.Word2Vec(workers=num_workers, \\\n",
    "                size=num_features, min_count = min_word_count, \\\n",
    "                window = context, sample = downsampling)\n",
    "\n",
    "    W2Vmodel.build_vocab([x for x in tqdm(X_train)])\n",
    "    W2Vmodel.train([x for x in tqdm(X_train)], \\\n",
    "                total_examples=W2Vmodel.corpus_count, epochs=W2Vmodel.epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildWordVector(model, tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError: \n",
    "            continue\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if features_extractor == 'word2Vec':\n",
    "    f_train = np.concatenate([buildWordVector(W2Vmodel, z, num_features) for z in map(lambda x: x, X_train)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of train features:: {}'.format(f_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving train features : \n",
    "\n",
    "if year != -1: np.save('./f_train'+'_'+str(year)+'.npy',f_train)\n",
    "else: np.save('./f_train.npy',f_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading train features : \n",
    "\n",
    "if year != -1: f_train = np.load('./f_train'+'_'+str(year)+'.npy')\n",
    "else:          f_train = np.load('./f_train.npy')\n",
    "\n",
    "f_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import calinski_harabaz_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans,MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "cali = []\n",
    "kmax = 25\n",
    "for i in range(2, kmax):\n",
    "    kmeans = MiniBatchKMeans(n_clusters = i, init = 'k-means++',\n",
    "                             n_init=1,init_size= int(1e3),\n",
    "                             random_state = 42,batch_size=int(1e3))\n",
    "    kmeans.fit(f_train)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    ypred = kmeans.predict(f_train)\n",
    "    cali.append(calinski_harabaz_score(f_train,ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(2, kmax), cali)\n",
    "plt.title('Calinski Harabaz Score')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Score')\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(2, kmax), wcss)\n",
    "plt.title('The Elbow Method (WCSS)')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Score')\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if year != -1: plt.savefig('./ElbowRule'+'_'+str(year)+'.png')\n",
    "else:          plt.savefig('./ElbowRule.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results : \n",
    "\n",
    "2012    4\n",
    "\n",
    "2013    4\n",
    "\n",
    "2014    4\n",
    "\n",
    "Todos   4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def elbow_rule(f_train, max_nb_cluster = 10, distortions_method='euclidean', plot=True):\n",
    "\n",
    "    # k means determine k\n",
    "    distortions = []\n",
    "\n",
    "    for k in range(1,max_nb_cluster):\n",
    "        print('Training K-means models for {} cluster/s...'.format(k))\n",
    "        kmeanModel = KMeans(n_clusters=k).fit(f_train)\n",
    "        kmeanModel.fit(f_train)\n",
    "        if distortions_method == 'euclidean':\n",
    "            distortions.append(sum(np.min(cdist(f_train, kmeanModel.cluster_centers_, 'euclidean'), \\\n",
    "                                          axis=1)) / f_train.shape[0])\n",
    "        #elif other distortion evaluation\n",
    "        \n",
    "    if plot:\n",
    "        # Plot the elbow\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.plot(range(1,max_nb_cluster), distortions, 'bx-')\n",
    "        plt.xlabel('k')\n",
    "        plt.ylabel('Distortion')\n",
    "        plt.title('Elbow Method')\n",
    "        plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = MiniBatchKMeans(n_clusters = 4, init = 'k-means++',\n",
    "                         n_init=1,init_size= int(1e3),\n",
    "                         random_state = 42,batch_size=int(1e3))\n",
    "kmeans.fit(f_train)\n",
    "ypred = kmeans.predict(f_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def plot_pca(f_train, y_train, nb_clusters = 4):\n",
    "\n",
    "    palette = np.array(sns.color_palette(\"hls\", nb_clusters))\n",
    "\n",
    "    pca = PCA(n_components=3)\n",
    "    result = pca.fit_transform(f_train)\n",
    "    \n",
    "\n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    ax.scatter(result[:, 0], result[:, 1],result[:, 2],\n",
    "               s=40, c=palette[ypred.astype(np.int)])\n",
    "\n",
    "    #    plt.figure(figsize=(8, 8))     \n",
    "    #    plt.scatter(result[:, 0], result[:, 1], lw=0, s=40,\n",
    "    #                    c=palette[y_train.astype(np.int)])\n",
    "\n",
    "    plt.title('Visualization PCA')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca(f_train, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words visualization using t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh.plotting as bp\n",
    "from sklearn.manifold import TSNE\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "\n",
    "\n",
    "\n",
    "def plot_tSNE(model,n_samples = 5000):\n",
    "\n",
    "    \n",
    "    #https://www.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm\n",
    "\n",
    "    output_notebook()\n",
    "    fig = bp.figure(plot_width=700, plot_height=600, title=\"A map of \" + str(n_samples) + \" word vectors\",\n",
    "        tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "        x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "\n",
    "\n",
    "    word_vectors = [model[w] for w in model.wv.vocab.keys()][:n_samples]\n",
    "    #word_vectors = [token for token in f_matrix_train][0:n_samples]\n",
    "    word_centroid_map = dict(zip( W2Vmodel.wv.index2word, ypred ))\n",
    "\n",
    "\n",
    "\n",
    "    tsne_model = TSNE(n_components=2, verbose=1, random_state=23)\n",
    "    tsne_w2v = tsne_model.fit_transform(word_vectors)\n",
    "\n",
    "    tsne_df = pd.DataFrame(tsne_w2v, columns=['x', 'y'])\n",
    "    tsne_df['words'] = [k for k in model.wv.vocab.keys()][:n_samples]\n",
    "\n",
    "    fig.scatter(x='x', y='y', source=tsne_df)\n",
    "    hover = fig.select(dict(type=HoverTool))\n",
    "    hover.tooltips={\"word\": \"@words\"}\n",
    "    show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tSNE(W2Vmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing words in clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "# a cluster number                                                                                            \n",
    "word_centroid_map = dict(zip( W2Vmodel.wv.index2word, ypred ))\n",
    "\n",
    "# For the first 5 clusters\n",
    "for cluster in range(0,4):\n",
    "    \n",
    "    print (f\"\\n Cluster {cluster}\")\n",
    "    words = []\n",
    "    for key, value in word_centroid_map.items():\n",
    "        if( value == cluster ):\n",
    "            words.append(key)\n",
    "\n",
    "    print(f'{words[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(ypred,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp =np.concatenate([f_train,ypred.reshape(-1,1)],axis=1)\n",
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var=[]\n",
    "for cluster in range(4): \n",
    "    var.append(np.var(tmp[tmp[:,-1]==cluster]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plt.scatter(range(0, 4), var)\n",
    "plt.title('Variance within cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ticklabel_format(style='plain',axis='x',useOffset=False)\n",
    "plt.ylabel('Variance')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.savefig('./Variance_4C.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(data):\n",
    "    corpus = []\n",
    "    for i in data:\n",
    "        for j in i:\n",
    "            corpus.append(j)\n",
    "    return corpus    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordCloud(corpus):\n",
    "    \n",
    "    wordCloud = WordCloud(background_color='white',\n",
    "                              stopwords=STOPWORDS,\n",
    "                              width=3000,\n",
    "                              height=2500,\n",
    "                              random_state=42\n",
    "                         ).generate(str(text['train'].data['headline_text_data']))\n",
    "    return wordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procTextCorpus = get_corpus(text['train'].data['headline_text_data'])\n",
    "rawTextCorpus = get_corpus(text['train'].data['headline_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('muted')\n",
    "sns.set_context(\"notebook\", font_scale=1.5,\n",
    "                rc={\"lines.linewidth\": 2.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procWordCloud = get_wordCloud(procTextCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.imshow(procWordCloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_print = 5 # change the number to analyse the # most common words\n",
    "\n",
    "word_counter = collections.Counter(procTextCorpus)\n",
    "for word, count in word_counter.most_common(n_print):\n",
    "    print(word, \": \", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,5))\n",
    "plt.subplot(1,2,1)\n",
    "\n",
    "sns.distplot(text['train'].data['nb_words'],hist=True, kde=False, bins=10, fit=norm)\n",
    "plt.title(\"Distribution of words in headline news\")\n",
    "plt.xlabel('Number of words in headline news')\n",
    "\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "lst = word_counter.most_common(n_print)\n",
    "df = pd.DataFrame(lst, columns = ['Word', 'Count'])\n",
    "plt.title('Most frequent words')\n",
    "df.plot(kind=\"barh\",x='Word',y='Count', ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
