{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing librares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from tqdm import tqdm\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from textDataset import *\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import calinski_harabaz_score\n",
    "from sklearn.cluster import KMeans,MiniBatchKMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_count = 2*multiprocessing.cpu_count()-1\n",
    "print('Number of CPUs: {}'.format(cpu_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path2data = '../../'\n",
    "path2data = '../data/news_headlines/'\n",
    "\n",
    "text = {\n",
    "        'train': TextDataset(path2data, extension='.csv', sep=',', is_train = True),\n",
    "}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'headline_text'\n",
    "text['train'].process_data(col = col, remove_stopw = True, remove_tags=False, lemmalize = True, stem = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dataset lenght: {}'.format(len(text['train'].data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "text['train'].data.publish_date = pd.to_datetime(text['train'].data.publish_date.astype(str),format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text['train'].data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "text['train'].data.publish_date.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "text['train'].data.publish_date.dt.year.value_counts().plot(kind = 'barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_extractor = 'word2Vec'\n",
    "#features_extractor = 'bow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildWordVector(model, tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError: \n",
    "            continue\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://www.kaggle.com/c/word2vec-nlp-tutorial#part-3-more-fun-with-word-vectors\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "    # Divide the result by the number of words to get the average\n",
    "    if nwords == 0:\n",
    "        nwords = 1\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://www.kaggle.com/c/word2vec-nlp-tutorial#part-3-more-fun-with-word-vectors\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews), num_features), dtype=\"float32\")\n",
    "    counter = 0\n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "        # Call the function (defined above) that makes average feature vectors\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(X_train, features_extractor = 'word2Vec', max_gram = 2, average_features = True):\n",
    "\n",
    "    print('Features extractor: {}'.format(features_extractor))\n",
    "\n",
    "    if features_extractor == 'bow':\n",
    "\n",
    "        print('Counting ocorrences of words. Ngram-range: {}...'.format(str((1,max_gram))))\n",
    "        vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                                     max_features = 300, ngram_range=(1,max_gram)) \n",
    "        f_train = vectorizer.fit_transform([\" \".join(x) for x in X_train])\n",
    "\n",
    "        #downscale weights for words using tf–idf: “Term Frequency times Inverse Document Frequency”.\n",
    "        print('Words downscaling using TF-IDF...')\n",
    "\n",
    "        tfidf_transformer = TfidfTransformer()\n",
    "        f_train = tfidf_transformer.fit_transform(f_train)\n",
    "        \n",
    "        model = vectorizer\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Set values for various parameters\n",
    "        num_features = 300    # Word vector dimensionality                      \n",
    "        min_word_count = 50   # Minimum word count                        \n",
    "        num_workers = cpu_count  # Number of threads to run in parallel\n",
    "        context = 10          # Context window size                                                                                    \n",
    "        downsampling = 1e-3   # Downsample setting for frequent words (default value)\n",
    "\n",
    "\n",
    "        print('Creating Word2Vec Model...')\n",
    "        model = word2vec.Word2Vec(workers=num_workers, \\\n",
    "                    size=num_features, min_count = min_word_count, \\\n",
    "                    window = context, sample = downsampling)\n",
    "\n",
    "\n",
    "        model.build_vocab(X_train)\n",
    "        model.train(X_train, \\\n",
    "                    total_examples=model.corpus_count, epochs=model.epochs)\n",
    "        \n",
    "        \n",
    "        if average_features:\n",
    "            f_train = getAvgFeatureVecs( X_train, model, num_features )\n",
    "        else:\n",
    "            f_train = np.concatenate([buildWordVector(model, z, num_features) for z in map(lambda x: x, X_train)])\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        f_train = scaler.fit_transform(f_train)\n",
    "                \n",
    "    return f_train, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nb_clusters(f_train, kmax = 25, plot=True, path2save = ''):\n",
    "    wcss = []\n",
    "    cali = []\n",
    "    \n",
    "    print('Max number of clusters for MiniBatchKMeans: {}'.format(kmax))\n",
    "    for i in tqdm(range(2, kmax)):\n",
    "        kmeans = MiniBatchKMeans(n_clusters = i, init = 'k-means++',\n",
    "                                 n_init=1,init_size= int(1e3),\n",
    "                                 random_state = 42,batch_size=int(1e3))\n",
    "        kmeans.fit(f_train)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "        ypred = kmeans.predict(f_train)\n",
    "        cali.append(calinski_harabaz_score(f_train,ypred))\n",
    "        \n",
    "    if plot:\n",
    "\n",
    "        fig = plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(range(2, kmax), cali, '-o')\n",
    "        plt.title('Calinski Harabaz Score')\n",
    "        plt.xlabel('Number of clusters')\n",
    "        plt.ylabel('Score')\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(range(2, kmax), wcss, '-o')\n",
    "        plt.title('The Elbow Method (WCSS)')\n",
    "        plt.xlabel('Number of clusters')\n",
    "        plt.ylabel('Score')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        fig.savefig(path2save)\n",
    "    return wcss, cali"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cluster_variance(f_train, n_clusters=4):\n",
    "\n",
    "    kmeans = MiniBatchKMeans(n_clusters = n_clusters, init = 'k-means++',\n",
    "                                 n_init=1,init_size= int(1e3),\n",
    "                                 random_state = 42,batch_size=int(1e3))\n",
    "    kmeans.fit(f_train)\n",
    "        \n",
    "    ypred = kmeans.predict(f_train)\n",
    "\n",
    "    #np.unique(ypred,return_counts=True)\n",
    "    tmp =np.concatenate([f_train,ypred.reshape(-1,1)],axis=1)\n",
    "    var=[]\n",
    "    for cluster in range(n_clusters): \n",
    "        var.append(np.var(tmp[tmp[:,-1]==cluster]))\n",
    "\n",
    "    return var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(data):\n",
    "    corpus = []\n",
    "    for i in data:\n",
    "        for j in i:\n",
    "            corpus.append(j)\n",
    "    return corpus    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordCloud(corpus):\n",
    "    \n",
    "    wordCloud = WordCloud(background_color='white',\n",
    "                              stopwords=STOPWORDS,\n",
    "                              width=3000,\n",
    "                              height=2500,\n",
    "                              max_words=200,\n",
    "                              random_state=42\n",
    "                         ).generate(str(corpus))\n",
    "    return wordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_most_frequent_words(corpus, n_print = 5):\n",
    "\n",
    "    word_counter = collections.Counter(procTextCorpus)\n",
    "    for word, count in word_counter.most_common(n_print):\n",
    "        print(word, \": \", count)\n",
    "    return word_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_lst = ['./figs/elbow/', './figs/qualitative_analysis/',\n",
    "                 './figs/quantitative_analysis/']\n",
    "for directory in directory_lst:    \n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster analysis for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "min_year = text['train'].data.publish_date.dt.year.min()\n",
    "max_year = text['train'].data.publish_date.dt.year.max()\n",
    "\n",
    "for y in np.arange(min_year, max_year):\n",
    "\n",
    "    X_train = text['train'].data[text['train'].data.publish_date.dt.year == y]\n",
    "    \n",
    "    print('Dataset length: {}. Year: {}'.format(len(X_train), y))\n",
    "    f_train, _ = extract_features(X_train[col + '_data'].values, average_features = True)\n",
    "    path2save = './figs/elbow/' + features_extractor + 'ElbowRule'+'_'+str(y)+'.png'\n",
    "    find_nb_clusters(f_train, kmax=20, path2save = path2save)\n",
    "    \n",
    "        \n",
    "    n_clusters= 5\n",
    "    \n",
    "    ## Quantitative Analysis\n",
    "    var = calculate_cluster_variance(f_train, n_clusters = n_clusters)\n",
    "    \n",
    "    fig = plt.figure(figsize=(8,4))\n",
    "    plt.scatter(range(0, n_clusters), var)\n",
    "    plt.title('Variance within cluster')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ticklabel_format(style='plain',axis='x',useOffset=False)\n",
    "    plt.ylabel('Variance')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    fig.savefig('./figs/quantitative_analysis/'+ features_extractor +'Variance_' + str(n_clusters) +'C.png')\n",
    "\n",
    "    ## Qualitative Analysis\n",
    "    procTextCorpus = get_corpus(X_train[col + '_data'])\n",
    "    procWordCloud = get_wordCloud(procTextCorpus)\n",
    "    n_print = 5\n",
    "    word_counter = count_most_frequent_words(procTextCorpus, n_print = n_print)\n",
    "                          \n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(procWordCloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    fig.savefig('./figs/qualitative_analysis/'+ features_extractor + 'word_clouds.png')\n",
    "    \n",
    "    \n",
    "    fig = plt.figure(figsize=(20,5))\n",
    "    plt.subplot(1,2,1)\n",
    "\n",
    "    sns.distplot(X_train['nb_words'],hist=True, kde=False, bins=10, fit=norm)\n",
    "    plt.title(\"Distribution of words in headline news\")\n",
    "    plt.xlabel('Number of words in headline news')\n",
    "\n",
    "    ax = fig.add_subplot(1,2,2)\n",
    "    lst = word_counter.most_common(n_print)\n",
    "    df = pd.DataFrame(lst, columns = ['Word', 'Count'])\n",
    "    plt.title('Most frequent words')\n",
    "    df.plot(kind=\"barh\",x='Word',y='Count', ax=ax)\n",
    "\n",
    "    fig.savefig('./figs/qualitative_analysis/'+ features_extractor + 'word_counter.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster analysis for whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = text['train'].data[col + '_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train, W2Vmodel = extract_features(X_train, average_features = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of train features:: {}'.format(f_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2save = './figs/elbow/'+ features_extractor +'ElbowRule.png'\n",
    "find_nb_clusters(f_train, kmax=20, path2save=path2save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = MiniBatchKMeans(n_clusters = n_clusters, init = 'k-means++',\n",
    "                         n_init=1,init_size= int(1e3),\n",
    "                         random_state = 42,batch_size=int(1e3))\n",
    "kmeans.fit(f_train)\n",
    "ypred = kmeans.predict(f_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words analysis in each clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "# a cluster number                                                                                            \n",
    "word_centroid_map = dict(zip( W2Vmodel.wv.index2word, ypred ))\n",
    "\n",
    "# For the first 5 clusters\n",
    "for cluster in range(0,4):\n",
    "    \n",
    "    print (f\"\\n Cluster {cluster}\")\n",
    "    words = []\n",
    "    for key, value in word_centroid_map.items():\n",
    "        if( value == cluster ):\n",
    "            words.append(key)\n",
    "\n",
    "    print(f'{words[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of words in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_map_series = pd.Series(word_centroid_map, index=word_centroid_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig  = plt.figure(figsize=(15,5))\n",
    "plt.title('Number of words in each cluster', fontsize = 16)\n",
    "words_map_series.value_counts().plot(kind = 'barh')\n",
    "plt.xlabel('Words count', fontsize = 16)\n",
    "plt.ylabel('Clusters', fontsize = 16)\n",
    "fig.savefig('./figs/quantitative_analysis/' + 'words_count_each_cluster_' + str(n_clusters) + 'C.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def plot_pca(f_train, y_train, nb_clusters = 4):\n",
    "\n",
    "    palette = np.array(sns.color_palette(\"hls\", nb_clusters))\n",
    "\n",
    "    pca = PCA(n_components=3)\n",
    "    result = pca.fit_transform(f_train)\n",
    "    \n",
    "\n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    ax.scatter(result[:, 0], result[:, 1],result[:, 2],\n",
    "               s=40, c=palette[ypred.astype(np.int)])\n",
    "\n",
    "    plt.title('Visualization PCA')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca(f_train, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words visualization using t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh.plotting as bp\n",
    "from sklearn.manifold import TSNE\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "\n",
    "\n",
    "\n",
    "def plot_tSNE(model,n_samples = 5000):\n",
    "\n",
    "    \n",
    "    #https://www.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm\n",
    "\n",
    "    output_notebook()\n",
    "    fig = bp.figure(plot_width=700, plot_height=600, title=\"A map of \" + str(n_samples) + \" word vectors\",\n",
    "        tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "        x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "\n",
    "\n",
    "    word_vectors = [model[w] for w in model.wv.vocab.keys()][:n_samples]\n",
    "    #word_vectors = [token for token in f_matrix_train][0:n_samples]\n",
    "    word_centroid_map = dict(zip( model.wv.index2word, ypred ))\n",
    "\n",
    "\n",
    "\n",
    "    tsne_model = TSNE(n_components=2, verbose=1, random_state=23)\n",
    "    tsne_w2v = tsne_model.fit_transform(word_vectors)\n",
    "\n",
    "    tsne_df = pd.DataFrame(tsne_w2v, columns=['x', 'y'])\n",
    "    tsne_df['words'] = [k for k in model.wv.vocab.keys()][:n_samples]\n",
    "\n",
    "    fig.scatter(x='x', y='y', source=tsne_df)\n",
    "    hover = fig.select(dict(type=HoverTool))\n",
    "    hover.tooltips={\"word\": \"@words\"}\n",
    "    show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tSNE(W2Vmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = calculate_cluster_variance(f_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plt.scatter(range(0, 4), var)\n",
    "plt.title('Variance within cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ticklabel_format(style='plain',axis='x',useOffset=False)\n",
    "plt.ylabel('Variance')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.savefig('./Variance_4C.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procTextCorpus = get_corpus(text['train'].data['headline_text_data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('muted')\n",
    "sns.set_context(\"notebook\", font_scale=1.5,\n",
    "                rc={\"lines.linewidth\": 2.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procWordCloud = get_wordCloud(procTextCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.imshow(procWordCloud)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "fig.savefig('./figs/all_data_word_clouds.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter = count_most_frequent_words(procTextCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,5))\n",
    "plt.subplot(1,2,1)\n",
    "\n",
    "sns.distplot(text['train'].data['nb_words'],hist=True, kde=False, bins=10, fit=norm)\n",
    "plt.title(\"Distribution of words in headline news\")\n",
    "plt.xlabel('Number of words in headline news')\n",
    "\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "lst = word_counter.most_common(n_print)\n",
    "df = pd.DataFrame(lst, columns = ['Word', 'Count'])\n",
    "plt.title('Most frequent words')\n",
    "df.plot(kind=\"barh\",x='Word',y='Count', ax=ax)\n",
    "fig.savefig('./figs/qualtitative_analysis/' + 'words_counter.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
