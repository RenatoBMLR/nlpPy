{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from textDataset import *\n",
    "\n",
    "from models import BoWClassifier, Tfid\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from trainner import TrainnerNLP\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import neighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "# We import seaborn to make nice plots.\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('muted')\n",
    "sns.set_context(\"notebook\", font_scale=1.5,\n",
    "                rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_count = 2*multiprocessing.cpu_count()-1\n",
    "print('Number of CPUs: {}'.format(cpu_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2data = '../data/sentiment_analysis/'\n",
    "\n",
    "text = {\n",
    "        'train': TextDataset(path2data + 'train/', extension='.tsv', sep='\\t', is_train = True),\n",
    "}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#col = 'headline_text'\n",
    "#col = 'comment_text'\n",
    "col = 'Phrase'\n",
    "text['train'].process_data(col = col, remove_stopw = True, remove_tags=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text['train'].data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = { 'train': text['train'].data[col + '_data'].values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sentences['train']\n",
    "#classes = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "classes = ['Sentiment']\n",
    "y = np.concatenate([np.array(i) for i in text['train'].data[classes].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_count=pd.value_counts(y, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Index = np.arange(1,len(class_count)+1)\n",
    "classes = ['negative','neutral','somewhat negative','somewhat positive','positive']\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.bar(Index,class_count,color = 'blue')\n",
    "plt.xticks(Index,classes,rotation=45)\n",
    "plt.ylabel('word_count')\n",
    "plt.xlabel('word')\n",
    "plt.title('Count of Moods')\n",
    "plt.bar(Index, class_count)\n",
    "for a,b in zip(Index, class_count):\n",
    "    plt.text(a, b, str(b) ,color='green', fontweight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data in training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 10   # Minimum word count                        \n",
    "num_workers = cpu_count  # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "\n",
    "# train model\n",
    "model = word2vec.Word2Vec(X_train, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://ahmedbesbes.com/sentiment-analysis-on-twitter-using-word2vec-and-keras.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "    # Divide the result by the number of words to get the average\n",
    "    if nwords == 0:\n",
    "        nwords = 1\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews), num_features), dtype=\"float32\")\n",
    "    counter = 0\n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "        # Call the function (defined above) that makes average feature vectors\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_matrix_train = getAvgFeatureVecs(X_train, model, num_features)\n",
    "f_matrix_valid = getAvgFeatureVecs(X_valid, model, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words visualization using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca(f_matrix_train, y_train, nb_clusters = 5):\n",
    "\n",
    "    palette = np.array(sns.color_palette(\"hls\", nb_clusters))\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    result = pca.fit_transform(f_matrix_train)\n",
    "    \n",
    "    plt.figure(figsize=(4, 4))     \n",
    "    plt.scatter(result[:, 0], result[:, 1], lw=0, s=40,\n",
    "                    c=palette[y_train.astype(np.int)])\n",
    "\n",
    "    plt.title('Visualization PCA')\n",
    "    plt.axis('off')\n",
    "    plt.axis('tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca(f_matrix_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words visualization using t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tSNE(f_matrix_train, y_train, nb_clusters=5):\n",
    "    \n",
    "    #https://www.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm\n",
    "    palette = np.array(sns.color_palette(\"hls\", nb_clusters))\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for sample in range(0, len(f_matrix_train)):\n",
    "        tokens.append(f_matrix_train[sample])\n",
    "        labels.append(y_train[sample])\n",
    "    \n",
    "    new_values = TSNE(random_state=23).fit_transform(tokens)\n",
    "        \n",
    "    plt.figure(figsize=(8, 8)) \n",
    "    plt.scatter(new_values[:,0],new_values[:,1], lw=0, s=40,\n",
    "                c=palette[y_train.astype(np.int)])\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.axis('tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tSNE(f_matrix_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining number of clusters - Elbow Rule\n",
    "###### When K increases, the centroids are closer to the clusters centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k means determine k\n",
    "distortions = []\n",
    "max_nb_cluster = 10\n",
    "for k in range(1,max_nb_cluster):\n",
    "    print('Training K-means models for {} cluster/s...'.format(k))\n",
    "    kmeanModel = KMeans(n_clusters=k).fit(f_matrix_train)\n",
    "    kmeanModel.fit(f_matrix_train)\n",
    "    distortions.append(sum(np.min(cdist(f_matrix_train, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / f_matrix_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the elbow\n",
    "plt.plot(range(1,max_nb_cluster), distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('Elbow Method')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time() # Start time\n",
    "num_clusters = 5\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "kmeans_clustering = KMeans( n_clusters = num_clusters )\n",
    "idx = kmeans_clustering.fit_predict( f_matrix_train )\n",
    "\n",
    "# Get the end time and print how long the process took\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print (\"Time taken for K Means clustering: \", elapsed, \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "# a cluster number                                                                                            \n",
    "word_centroid_map = dict(zip( model.wv.index2word, idx ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the first 5 clusters\n",
    "for cluster in range(0,num_clusters):\n",
    "    \n",
    "    print (\"\\nCluster %d\" % cluster)\n",
    "    words = []\n",
    "    for key, value in word_centroid_map.items():\n",
    "        if( value == cluster ):\n",
    "            words.append(key)\n",
    "    print (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = neighbors.KNeighborsClassifier(n_neighbors=num_clusters)\n",
    "print('Trainning K-Nearest Neighbors...')\n",
    "\n",
    "knn.fit(f_matrix_train, y_train)\n",
    "valid_score = knn.score(f_matrix_valid, y_valid)\n",
    "\n",
    "print('K-Nearest Neighbors - Score: {}'.format(valid_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning approach"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
